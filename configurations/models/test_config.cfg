[architecture]
    model_type = none
    inverse_relations = separate
    final_sentence_embedding = sentence_attention
    loss_type = fancy

[gcn]
    layers = 1
    embedding_dimension = 200
    message_hidden_dimension = 200
    gate_hidden_dimension = 200

[lstm]
    layers = 1
    embedding_dimension = 200
    attention_heads = 4
    static_word_embeddings = True

[other]
    final_hidden_dimensions = 400

[regularization]
    word_dropout = 0.0
    final_l2 = 0.001
    final_dropout = 0.2
    attention_dropout = 0.2
    gate_l1 = 0.0001
    gcn_dropout = 0.2
    gcn_l2 = 0.0001

[endpoint]
    type = csv
    file = data/toy-125/toy.graph
    disk_cache = data/toy-125/1neighbors.cache
    project_names = False

[indexes]
    relation_index_type = toy_default_relations:200
    vertex_index_type = toy_default_vertices:100
    word_index_type = toy_default_vertices:300
    pos_index_type = toy_default_pos:10
    relation_part_index_type = toy_default_relation_part:10

[training]
    subsampling = 10
    batch_size = 5
    max_iterations = 1000
    validate_every_n = 1
    report_loss_every_n = 5
    early_stopping = False
    filter_gold_labels = maximize_f1
    learning_rate = 0.001
    gradient_clipping = 1.0
    additional_matching_golds = 0.8

[testing]
    batch_size = 1

[dataset]
    train_file = data/toy-125/train.internal.uppercased_and_tagged.conll
    valid_file = data/toy-125/test.internal.uppercased_and_tagged.conll
    test_file = data/toy-125/test.internal.uppercased_and_tagged.conll
    transform_mention_scores = none